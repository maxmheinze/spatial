---
title: '**Spatial Economics -- Assignment 2**'
author: 
  - "Max Heinze (h11742049@s.wu.ac.at)"
  - "Jaime Miravet (h12235992@s.wu.ac.at)"
  - "Jan Trimmel (h11809096@s.wu.ac.at)"
date: "April 12, 2024"
output: 
  pdf_document:
    toc: true
    citation_package: biblatex
    includes:
      in_header: !expr file.path(rprojroot::find_rstudio_root_file(), "helper", "wrap_code.tex")
bibliography: references.bib
biblio-style: apa
header-includes: 
  - \usepackage{tcolorbox}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
  - \usepackage[default]{lato}
papersize: a4
geometry: margin = 2cm
urlcolor: DarkOrchid!65!black
citecolor: DarkOrchid!65!black
---

```{r, setup, include = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
library(showtext)
font_add_google("Lato", "Lato")
showtext_auto()
```

\vspace{2em}

\begin{tcolorbox}
\centering \itshape The executable code that was used in compiling the assignment is available on GitHub at \url{https://github.com/maxmheinze/spatial}.
\end{tcolorbox}

\newpage

# Task A

First, we load both the datafile and shapefile, select the countries of interest, and create the productivity growth rate variable.
```{r, warning = FALSE}
pacman::p_load(
  dplyr,
  ggplot2,
  sf,
  spdep,
  raster
)

#Load dataset and shapefile
load("./assignment2/data/data1.rda")
shp <- st_read(dsn = "./assignment2/data/EU27.shp", quiet = TRUE)

#Select variables of interest
vars <- c("IDb", "pr80b", "pr103b", "lninv1b", "lndens.empb")
data <- data1[vars]

#Create productivity growth variable for countries of interest
merged_shp <- left_join(shp, data, by=c("Id"="IDb"))
countries <- c("AT", "DE","ES","FR","IT","PT")
df <- merged_shp[grep(paste(countries, collapse = "|"),merged_shp$Id),]
df <- na.omit(df)
df$prgrowth <- ((df$pr103b-df$pr80b)/df$pr80b)*100
```

We create the productivty growth rate map:
```{r, echo=TRUE}
# Cut the productivity growth values into five segments to plot them
breaks <- c(-20,-10, 0, 15, 30, 50)
labels <- c("(-20,-10)","(-10,0)", "(0,15)","(15,30)","(30,50)")
df$prgrowth_bins <- cut(df$prgrowth, breaks = breaks, labels = labels)

#Visualization of productivity growth rate
bin_colors <- c("lightyellow", "yellow", "orange", "red","brown")
ggplot() +
  geom_sf(data = df, aes(fill = prgrowth_bins)) +
  scale_fill_manual(name = "%", 
                    breaks = labels, 
                    values = bin_colors) +
  labs(title = "Productivity Growth Rate (1980-2003)") +
  theme_minimal()
```

## Creating the weight matrices

### (1) Distance threshold matrix

```{r, warning=FALSE}
#Create centroids of regions
centr <- st_centroid(df)
coord <- st_coordinates(centr)
dist <- dnearneigh(coord, 0, 10000, row.names=df$Id)

#Minimum distance threshold so there are no disconnected regions
k1 <- knearneigh(coord, k=1)
k1 <- knn2nb(k1)
min.max <- max(unlist(nbdists(k1, coords=coord)))
min.max

#We use the obtained maximum value of the threshold
dist <- dnearneigh(coord, 0, min.max, row.names=df$Id)

#With this new distance threshold, each region has an average of 8.3 links.

w1 <- nb2mat(dist, style= "W")
```

It appears that setting a large maximum value in the distance threshold yields too many links per region. Therefore, we compute the minimum value of the max. distance threshold so that no region is isolated. We create a weight matrix in which each region has a positive value if its kth neighbor is below the maximum distance threshold (around 2.65) and 0 if it is above.

### (2) Smooth distance-decay matrix

We create a smooth distance-decay function so that the value decreases as distance between regions increases by setting $\delta$ = 0 in the exponential function $\exp[\delta d(i,j)]$. Then, we create a matrix by transforming the distance matrix that contains the distances between the centroids of the regions with the smooth distance-decay function 
```{r}
function0 <- function(x){
  exp(-0.4*x)
}


distance <- st_distance(centr)
g2 <- matrix(NA, nrow=nrow(distance), ncol=ncol(distance))

for(i in seq_along(distance)){
  g2[i]<-function0(distance[i])
}

#Finally, we row-normalize the matrix and set its diagonal to 0 to create a weight matrix.
w2 <- g2/rowSums(g2)
diag(w2) <- 0
```

### (3) Contiguity-based matrix

We construct a matrix such that in the row of each region each element will be positive for contiguous neighbors and 0 otherwise. 

```{r}
contiguity <- poly2nb(df, row.names=df$Id, queen=TRUE)

w3 <- nb2mat(contiguity, style = "W", zero.policy = TRUE) 
#zero.policy is set to TRUE since there are regions with no contiguous neighbors 
```

## Comparing the matrices

First, we can compute the sparsity of each matrix (proportion of zero elements).
```{r}
sp1 <- sum(w1 == 0)/length(w1)
sp2<- sum(w2 == 0)/length(w2)
sp3 <- sum(w3 == 0)/length(w3)
sparsity <- c(sp1, sp2, sp3)
names(sparsity) <- c("W1", "W2", "W3")
print(sparsity)
```

We observe that the elements of the matrices based on a distance threshold and on contiguity are mostly zero.The matrix based on a smooth distance-decay function has a much smaller proportion of zero elements.
On the one hand, given the small distance threshold imposed and the large number of regions, it's not surprising that $W_1$ and $W_3$ have mostly zero elements, representing no link with most neighbors. On the other hand, since we use a decaying exponential function to construct $W_2$, even the regions that are furthest away from each other have a positive value. Therefore, the only zero elements are diagonal elements.

Then we compute their eigenvalues.
```{r}
eigenw1 <- eigen(w1)$value
eigenw2 <- eigen(w2)$value
eigenw3 <- eigen(w3)$value

#Since each matrix has 103 eigenvalues, we focus on the largest 5.
eigentop1 <- eigenw1[1:5]
eigentop2 <- eigenw2[1:5]
eigentop3 <- eigenw3[1:5]

eigenvalues <- cbind(eigentop1, eigentop2, eigentop3)
print(eigenvalues)
```

We observe that the top 5 eigenvalues of $W_1$ and $W_3$ are larger than those of $W_2$. This shows there is stronger spatial autocorrelation in $W_1$ and $W_3$. On the other hand, the eigenvalues of $W_1$ and $W_3$ appear to decrease gradually, whereas those of $W_2$ decrease more rapidly. This indicates a faster decay in spatial autocorrelation with distance in $W_2$ compared to the other two matrices.

Now, in order to analyze the matrices from a graph theory perspective, we convert the weight matrices into adjacency (binary) matrices.
```{r}
d1 <- ifelse(w1 > 0, 1, 0)
d2 <- ifelse(w2 > 0, 1, 0)
d3 <- ifelse(w3 > 0, 1, 0)
```

Once we have adjacency matrices, we can compute their degree distribution. Note that since $W_2$ has no zero off-diagonal elements, every region is connected to each other, so there is no point on computing the degree distribution of $D_2$.

```{r}
node_degrees1 <- rowSums(d1)
node_degrees3 <- rowSums(d3)

hist(node_degrees1, main = "Degree Distribution D1", xlab = "Node Degree", ylab = "Frequency", xlim = c(0,25), ylim = c(0,25))
hist(node_degrees3, main = "Degree Distribution D3", xlab = "Node Degree", ylab = "Frequency", xlim = c(0,15), ylim = c(0,25))
```

The adjacency matrix obtained when transforming $W_1$, $D_1$, shows a degree distribution more spread and skewed to the right, whereas $D_3$ (transformed $W_3$) shows a more concentrated degree distribution with hardly no skeewness.

### Plotting the matrices

```{r}
v1 <- raster(w1)
v2 <- raster(w2)
v3 <- raster(w3)
plot(v1)
plot(v2)
plot(v3)
```

We observe that $W_1$ and $W_3$ represent a network there are less connections between regions, but the existing links are stronger than those of $W_2$.The smooth distance-decay matrix $W_2$, on the other hand, represents a much more interconnected network, where each region is connected to each other but the links are weaker on average.

$W_2$ can help us visualize the clusters in the network. To do so, we must take into account how the countries are ordered in the matrix. The order (from first rows(columns) to last) is Austria, Germany, Spain, France, Italy and Portugal. (This is because the region codes are ordered alphabetically (AT, DE, ES, FR, IT, PT). Looking at $W_2$ we observe the biggest cluster is located among the Austrian and German regions (top-left corner). Then, the smaller cluster in the middle represents Spain. As we can see, Spanish regions have a almost non-existing link with Austria and Germany, and their stronger links are related to Portugal (middle of the bottom (or right) part). Then, in the diagonal below Spain we see France. We can see how it is the country with more connections to the rest of countries. Below in the diagonal we observe the cluster formed by the Italian regions. We see that their strongest foreign links are with Austrian regions. Finally, in the very bottom-right corner we see Portugal. It is clear how Portuguese regions are isolated from all countries except Spain.

## Computing a measure of spatial autocorrelation for productivity growth

To measure spatial autocorrelation, we will compute a Global Moran's I statistic for each matrix.

```{r}
#(1) Distance threshold matrix
w1listw <- mat2listw(w1, style = "W") #We tranform w1 into a listw so that moran.test() works
i1 <- moran.test(df$prgrowth, w1listw)
i1 <- i1$estimate["Moran I statistic"]
#(2) Smooth distance-decay  matrix
w2listw <- mat2listw(w2, style = "W")
i2 <- moran.test(df$prgrowth, w2listw)
i2 <- i2$estimate["Moran I statistic"]
#(3) Contiguity matrix
w3listw <- mat2listw(w3, style = "W", zero.policy = TRUE)
i3 <- moran.test(df$prgrowth, w3listw)
i3 <- i3$estimate["Moran I statistic"]

#I's statistics comparison
I_score_comparison <- c(i1, i2, i3)
names(I_score_comparison) <- c("w1", "w2", "w3")
print(I_score_comparison)
```

The Global Moran's I score can range from -1 to 1, and it measures spatial autocorrelation, indicating the degree of similarity between neighboring regions in terms of  productivity growth rate. We observe that $W_1$ and $W_3$ have a similar value around 0.55, indicating a moderate positive spatial autocorrelation between the regions. The I statistic of matrix $W_2$ is somewhat smaller, implying a lower spatial autocorrelation, though it still shows signs of clustering.

## OLS regression

```{r}
#pr80b, pr103b: Productivity of the region in 1980 and 2003
#lninv1b: log of investment
#lndens.empb: log of densitiy of employment

#We run the regression and store the residuals.
ols <- lm(prgrowth ~ pr80b + lninv1b + lndens.empb, data = df)
residuals <- residuals(ols)
```

We want to observe if errors are spatially autocorrelated, i.e., if errors of similar size come from regions that are closer in space. To do that, we can compute the spatially lagged errors of the regression. That is, we can multiply our desired weight matrix by the vector of residuals. Note that $Wû$ shows the average value of the residuals of the neighbors of each region. This can show how clustered residuals are in space.

```{r}
lagged_residuals_w1 <- lag.listw(w1listw, residuals)
lagged_residuals_w2 <- lag.listw(w2listw, residuals)
lagged_residuals_w3 <- lag.listw(w3listw, residuals)
```

To visually check if the residuals are autocorrelated, we can create scatter plots which relate each residuals to its spatially lagged residual. The intuition behind this is that for each region, its residual is compared to the average residual value of its neighbors. If there is spatial autocorrelation, we would expect each residual ($û_i$) to be of similar size to the the average residual value of its neighbors ($Wû_i$).

```{r}
plot(residuals, lagged_residuals_w1, main = "w1", xlab = "û", ylab = "Wû")
plot(residuals, lagged_residuals_w2, main = "w2", xlab = "û", ylab = "Wû")
plot(residuals, lagged_residuals_w3, main = "w3", xlab = "û", ylab = "Wû")
```

For all three weight matrices, we observe a slight positive relationship between residuals and their spatially lagged counterparts, specially when using $W_3$, implying certain spatial autocorrelation between residuals.

To statistically check for spatial autocorrelation, we can compute once again Global Moran's I using each weight matrix, this time analyzing the residuals instead of productivity growth.

```{r}
iw1 <- moran.test(residuals, w1listw)
iw1 <- iw1$estimate["Moran I statistic"]

iw2 <- moran.test(residuals, w2listw)
iw2 <- iw2$estimate["Moran I statistic"]

iw3 <- moran.test(residuals, w3listw)
iw3 <- iw3$estimate["Moran I statistic"]

I_score_comparison2 <- c(iw1, iw2, iw3)
names(I_score_comparison2) <- c("w1", "w2", "w3")
print(I_score_comparison2)
```

The  statistics related to $W_1$ and $W_3$ imply a moderate level of clustering of errors, whereas if we use $W_2$ to check for spatial autocorrelation, we would obtain a value indicating less clustering. The presence of spatial dependence across errors implies that the assumption of independent errors is violated. If we were to run an OLS regression, our estimates would be biased and inconsistent.If we suspect that the errors are spatially autocorrelated, we can use the spatial error model (SEM), using a weight matrix to account for the autocorrelation.

On the other hand, if there is spatial dependence between the observations of interest (in our case productivity growth rate), the estimates obtained from a regression will not accurately capture the true effect of the independent variable(s), since effects related to spatial characteristics will be mistakenly attributed to the parameter.
In that case, we could estimate a spatial autoregressive model (SAR), in which the neighbors' characteristics are also considered when estimating the parameters.

\newpage

# Task B

## Nice Maps

We create four maps:

```{r, out.width = "70%", fig.align = "center", echo = FALSE, dev = "png", dpi = 300}

# Header ------------------------------------------------------------------

pacman::p_load(
  tidyverse,
  tmap,
  haven,
  foreign,
  magrittr,
  sf,
  stargazer,
  conleyreg,
  estimatr,
  fixest
)


# Prepare Dataframe -------------------------------------------------------

litr <- read_dta("./assignment2/data/literacy_Arg-Bra-Par.dta", encoding = "ISO-8859-1")

shp0 <- read_sf("./assignment2/data/task_b_shapefile_0.shp")
shp1 <- read_sf("./assignment2/data/task_b_shapefile_1.shp")
shp2 <- read_sf("./assignment2/data/task_b_shapefile_2.shp")

spli <- shp2 %>%
  left_join(litr, by = c("NAME_2" = "muni"))


# Literacy Map ------------------------------------------------------------

tm_shape(spli) + 
  tm_fill("literacy", palette = "Reds", style = "equal", n = 10) + 
  tm_borders(col = "#BBBBBB") + 
  tm_shape(shp1) + 
  tm_borders(col = "#444444") + 
  tm_text("NAME_1", size = 0.75, col = "black", bg.color = "#FFFFFF") +
  tm_shape(shp0) + 
  tm_borders(col = "#000000") +
  tm_compass(position = c("right", "top"), size = 2) +
  tm_scale_bar(position = c("right", "bottom"), text.size = 0.5) +
  tm_layout(title = "Literacy Rate by Municipality", 
            title.bg.color = "white", 
            legend.position = c("left", "bottom"),
            legend.bg.color = "white", 
            frame = FALSE,
            fontfamily = "Lato")


# Population Density Map --------------------------------------------------

tm_shape(spli) + 
  tm_fill("popd", palette = "Blues", style = "quantile", n = 10) + 
  tm_borders(col = "#BBBBBB") + 
  tm_shape(shp1) + 
  tm_borders(col = "#444444") + 
  tm_text("NAME_1", size = 0.75, col = "black", bg.color = "#FFFFFF") +
  tm_shape(shp0) + 
  tm_borders(col = "#000000") +
  tm_compass(position = c("right", "top"), size = 2) +
  tm_scale_bar(position = c("right", "bottom"), text.size = 0.5) +
  tm_layout(title = "Population Density by Municipality", 
            title.bg.color = "white", 
            legend.position = c("left", "bottom"),
            legend.bg.color = "white", 
            frame = FALSE,
            fontfamily = "Lato")


# Jesuit Distance Map -----------------------------------------------------

tm_shape(spli) + 
  tm_fill("distmiss", palette = "-Purples", style = "equal", n = 10) + 
  tm_borders(col = "#BBBBBB") + 
  tm_shape(shp1) + 
  tm_borders(col = "#444444") + 
  tm_text("NAME_1", size = 0.75, col = "black", bg.color = "#FFFFFF") +
  tm_shape(shp0) + 
  tm_borders(col = "#000000") +
  tm_compass(position = c("right", "top"), size = 2) +
  tm_scale_bar(position = c("right", "bottom"), text.size = 0.5) +
  tm_layout(title = "Distance from Jesuit Missions by Municipality", 
            title.bg.color = "white", 
            legend.position = c("left", "bottom"),
            legend.bg.color = "white", 
            frame = FALSE,
            fontfamily = "Lato")

# Franciscan Distance Map -------------------------------------------------

tm_shape(spli) + 
  tm_fill("distfran", palette = "-Greens", style = "equal", n = 10) + 
  tm_borders(col = "#BBBBBB") + 
  tm_shape(shp1) + 
  tm_borders(col = "#444444") + 
  tm_text("NAME_1", size = 0.75, col = "black", bg.color = "#FFFFFF") +
  tm_shape(shp0) + 
  tm_borders(col = "#000000") +
  tm_compass(position = c("right", "top"), size = 2) +
  tm_scale_bar(position = c("right", "bottom"), text.size = 0.5) +
  tm_layout(title = "Distance from Franciscan Missions by Municipality", 
            title.bg.color = "white", 
            legend.position = c("left", "bottom"),
            legend.bg.color = "white", 
            frame = FALSE,
            fontfamily = "Lato")
```

## Replication of Table 2

Below are code and results of our replication of Table 2 from \textcite{valencia2019}. Since the original author used Stata's robust standard errors, a notorious problem for replication in R, we specifically report Stata-style standard errors in the table below using the `starprep` function from the `estimatr` package. Using these standard errors, we can reproduce both coefficients and standard errors exactly.

```{r}

# Replicate Results -------------------------------------------------------
col1 <- lm(illiteracy ~ distmiss + lati + longi + corr + ita + mis + mis1, data = litr)
col2 <- lm(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr + ita + mis + mis1, data = litr)

litr_bra <- subset(litr, country == "BRA")
col3 <- lm(illiteracy ~ distmiss + lati + longi + as.factor(mesorregi), data = litr_bra)
col4 <- lm(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + as.factor(mesorregi), data = litr_bra)

litr_arg <- subset(litr, country == "Argentina")
col5 <- lm(illiteracy ~ distmiss + lati + longi + corr, data = litr_arg)
col6 <- lm(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr, data = litr_arg)

litr_pry <- subset(litr, country == "Paraguay")
col7 <- lm(illiteracy ~ distmiss + ita, data = litr_pry)
col8 <- lm(illiteracy ~ distmiss + area + tempe + alti + preci + rugg + river + coast + ita, data = litr_pry)

```

```{r, echo = FALSE, results='asis'}
cat("\\begin{center}")
cat("\\scalebox{.9}{")



stargazer(col1, col2, col3, col4,
          type = "latex", header=FALSE, no.space = TRUE, column.sep.width = "3pt", float = FALSE,
          se = starprep(col1, col2, col3, col4, se_type = "stata"),
          omit.stat = "f")

cat("}")
cat("\\end{center}")
```

```{r, echo = FALSE, results='asis'}
cat("\\begin{center}")
cat("\\scalebox{.9}{")

stargazer(col5, col6, col7, col8,
          type = "latex", header=FALSE, no.space = TRUE, column.sep.width = "3pt", float = FALSE,
          column.labels = c("(5)", "(6)", "(7)", "(8)"),
          se = starprep(col5, col6, col7, col8, se_type = "stata"),
          omit.stat = "f")

cat("}")
cat("\\end{center}")
```

Next, we try to reproduce the Conley standard errors. We try two different approaches, first using the `conleyreg` package and then using the `fixest` package. \textcite{valencia2019} specifies a cutoff distance of 0.1 degrees. Both packages we use only allow us to specify the cutoff distance in kilometers, so for the sake of simplicity, we use the distance that 0.1 degrees equal at the equator, which is 6 nautical miles, or approximately 11.112 kilometers. We first print the results from the `conleyreg` package and then from the `fixest` package.

```{r}
lit1 <- litr %>%
  drop_na(lati, longi) %>%
  mutate(lat = lati,
         lon = longi)

col1c <- conleyreg(illiteracy ~ distmiss + lati + longi + corr + ita + mis + mis1, data = lit1,
           dist_cutoff = 11.112, lat = "lat", lon = "lon")
col2c <- conleyreg(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr + ita + mis + mis1, data = lit1,
           dist_cutoff = 11.112, lat = "lat", lon = "lon")

lit1_bra <- subset(lit1, country == "BRA")
col3c <- conleyreg(illiteracy ~ distmiss + lati + longi + as.factor(mesorregi), data = lit1_bra,
           dist_cutoff = 11.112, lat = "lat", lon = "lon")
col4c <- conleyreg(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + as.factor(mesorregi), data = lit1_bra,
           dist_cutoff = 11.112, lat = "lat", lon = "lon")

lit1_arg <- subset(lit1, country == "Argentina")
col5c <- conleyreg(illiteracy ~ distmiss + lati + longi + corr, data = lit1_arg,
           dist_cutoff = 11.112, lat = "lat", lon = "lon")
col6c <- conleyreg(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr, data = lit1_arg,
           dist_cutoff = 11.112, lat = "lat", lon = "lon")

lit1_pry <- subset(lit1, country == "Paraguay")
col7c <- conleyreg(illiteracy ~ distmiss + ita, data =  lit1_pry,
                   dist_cutoff = 11.112, lat = "lat", lon = "lon")
col8c <- conleyreg(illiteracy ~ distmiss + area + tempe + alti + preci + rugg + river + coast + ita, data =  lit1_pry,
                   dist_cutoff = 11.112, lat = "lat", lon = "lon")
```

```{r, echo = FALSE, results='asis'}
cat("\\begin{center}")
cat("\\scalebox{.9}{")

stargazer(col1c, col2c, col3c, col4c, type = "latex", header=FALSE, no.space = TRUE, column.sep.width = "3pt", float = FALSE)

cat("}")
cat("\\end{center}")
```

```{r, echo = FALSE, results='asis'}
cat("\\begin{center}")
cat("\\scalebox{.9}{")

stargazer(col5c, col6c, col7c, col8c, column.labels = c("(5)", "(6)", "(7)", "(8)"), model.numbers = FALSE, type = "latex", header=FALSE, no.space = TRUE, column.sep.width = "3pt", float = FALSE)

cat("}")
cat("\\end{center}")
```

```{r}
col1cf <- feols(illiteracy ~ distmiss + lati + longi + corr + ita + mis + mis1, data = litr, 
                 vcov_conley(lat = "lati", lon = "longi", cutoff = 11.112, distance = "spherical"))
col2cf <- feols(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr + ita + mis + mis1, data = litr, 
                 vcov_conley(lat = "lati", lon = "longi", cutoff = 11.112, distance = "spherical"))

col3cf <- feols(illiteracy ~ distmiss + lati + longi + as.factor(mesorregi), data = litr_bra, 
                 vcov_conley(lat = "lati", lon = "longi", cutoff = 11.112, distance = "spherical"))
col4cf <- feols(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + as.factor(mesorregi), data = litr_bra, 
                 vcov_conley(lat = "lati", lon = "longi", cutoff = 11.112, distance = "spherical"))

col5cf <- feols(illiteracy ~ distmiss + lati + longi + corr, data = litr_arg, 
                 vcov_conley(lat = "lati", lon = "longi", cutoff = 11.112, distance = "spherical"))
col6cf <- feols(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr, data = litr_arg, 
                 vcov_conley(lat = "lati", lon = "longi", cutoff = 11.112, distance = "spherical"))

col7cf <- feols(illiteracy ~ distmiss + ita, data = litr_pry, 
                 vcov_conley(lat = "lati", lon = "longi", cutoff = 11.112, distance = "spherical"))
col8cf <- feols(illiteracy ~ distmiss + area + tempe + alti + preci + rugg + river + coast + ita, data = litr_pry, 
                 vcov_conley(lat = "lati", lon = "longi", cutoff = 11.112, distance = "spherical"))
```

```{r, echo = FALSE, results='asis'}
cat("\\scalebox{.8}{")
etable(col1cf, col2cf, col3cf, col4cf, col5cf, col6cf, col7cf, col8cf, tex = TRUE)
cat("}")
```

Evidently, we could not reproduce the exact Conley standard errors reported in \textcite{valencia2019}. And the two packages yielded different errors even though we specified the same cutoff (11.112 kilometers) and the same method of distance calculation (spherical). 

## Ways to Use Distance

In his specification, Felipe \textcite{valencia2019} uses the following specification (Eq. 1 in the paper):

$$
Y_{2000,ij} = \alpha + \beta d(M_{ij}) + \gamma GEO_{ij}+\mu_j + \varepsilon_{ij},
$$

where $d(\cdot)$ is described as either being a dummy for the presence of a mission or the plain (i.e., untransformed) distance, although we do not fine examples of the former in any of the printed tables in the paper, so we will just be calling it the distance. The decaying impact of a mission can be plotted against distance as follows:

```{r, echo = FALSE, out.width = "50%", fig.align = "center"}
ggplot() +
  geom_function(col = "#995082", size = 1.5, fun = function(x) x) +
  xlim(0, 1000) +
  labs(x = "x", y = "d(x)") +
  theme_minimal() +
  theme(text = element_text(family = "Lato"))
```

If we view $d(\cdot)$ as _impact_ exerted by a mission, it might be more intuitive to plot the negative of it, so that a higher value corresponds to more impact.

```{r, echo = FALSE, out.width = "50%", fig.align = "center"}
ggplot() +
  geom_function(col = "#995082", size = 1.5, fun = function(x) -x) +
  xlim(0, 1000) +
  labs(x = "x", y = "-d(x)") +
  theme_minimal() +
  theme(text = element_text(family = "Lato"))
```

If we draw a map and plot distance from a mission using this function, we get the same map as above.

```{r, echo = FALSE, out.width = "70%", fig.align = "center", dev = "png", dpi = 300}
tm_shape(spli) + 
  tm_fill("distmiss", palette = "-Purples", style = "equal", n = 10) + 
  tm_borders(col = "#BBBBBB") + 
  tm_shape(shp1) + 
  tm_borders(col = "#444444") + 
  tm_text("NAME_1", size = 0.75, col = "black", bg.color = "#FFFFFF") +
  tm_shape(shp0) + 
  tm_borders(col = "#000000") +
  tm_compass(position = c("right", "top"), size = 2) +
  tm_scale_bar(position = c("right", "bottom"), text.size = 0.5) +
  tm_layout(title = "Distance from Jesuit Missions, Linear", 
            title.bg.color = "white", 
            legend.position = c("left", "bottom"),
            legend.bg.color = "white", 
            frame = FALSE,
            fontfamily = "Lato")
```


Letting $d$ be

$$
d: \mathbb{R} \to \mathbb{R}, \quad x \mapsto x
$$

means that we attribute the same change in impact to moving from 0km from the treatment position to 100km from it, as to moving from 900km distance to 1000km. This is quite an assumption. Alternatively, we could use the **logarithm** of the distance, 

$$
d: \mathbb{R} \to \mathbb{R}, \quad x \mapsto \mathrm{log}(x),
$$

like this:

```{r, echo = FALSE, out.width = "50%", fig.align = "center"}
ggplot() +
  geom_function(col = "#995082", size = 1.5, fun = function(x) -log(x)) +
  xlim(0, 1000) +
  labs(x = "x", y = "-d(x)") +
  theme_minimal() +
  theme(text = element_text(family = "Lato"))
```

This gives the following map.

```{r, echo = FALSE, out.width = "70%", fig.align = "center", dev = "png", dpi = 300}
tm_shape(mutate(spli, distmissx = log(distmiss))) + 
  tm_fill("distmissx", palette = "-Purples", style = "equal", n = 10) + 
  tm_borders(col = "#BBBBBB") + 
  tm_shape(shp1) + 
  tm_borders(col = "#444444") + 
  tm_text("NAME_1", size = 0.75, col = "black", bg.color = "#FFFFFF") +
  tm_shape(shp0) + 
  tm_borders(col = "#000000") +
  tm_compass(position = c("right", "top"), size = 2) +
  tm_scale_bar(position = c("right", "bottom"), text.size = 0.5) +
  tm_layout(title = "Distance from Jesuit Missions, Logarithmic", 
            title.bg.color = "white", 
            legend.position = c("left", "bottom"),
            legend.bg.color = "white", 
            frame = FALSE,
            fontfamily = "Lato")
```

We can see that unlike in the linear case, the differences between distances at a greater length from the locations of treatment are weighted down, whereas distance differences closer to the mission are exaggerated. It could make sense to use such a specification instead of a linear one if we suspect impacts of a mission to be roughly comparable between places that are 400 or 500 kilometers from the closest mission, or if we are concerned that portions of Rio Grande do Sul are included in the analysis that are much farther from where Jesuits went than portions of Argentinian or Paraguayan states that were not included.

Alternatively, we could consider a **negative exponential** specification,

$$
d: \mathbb{R} \to \mathbb{R}, \quad x \mapsto \mathrm{exp}(-\lambda x),
$$

which is plotted below for $\lambda = \tfrac{1}{500}$:

```{r, echo = FALSE, out.width = "50%", fig.align = "center"}
ggplot() +
  geom_function(col = "#995082", size = 1.5, fun = function(x) exp(-x/200)) +
  xlim(0, 1000) +
  labs(x = "x", y = "d(x)") +
  theme_minimal() +
  theme(text = element_text(family = "Lato"))
```

Applying to our data, it looks like this:

```{r, echo = FALSE, out.width = "70%", fig.align = "center", dev = "png", dpi = 300}
tm_shape(mutate(spli, distmissx = exp(-distmiss/200))) + 
  tm_fill("distmissx", palette = "Purples", style = "equal", n = 10) + 
  tm_borders(col = "#BBBBBB") + 
  tm_shape(shp1) + 
  tm_borders(col = "#444444") + 
  tm_text("NAME_1", size = 0.75, col = "black", bg.color = "#FFFFFF") +
  tm_shape(shp0) + 
  tm_borders(col = "#000000") +
  tm_compass(position = c("right", "top"), size = 2) +
  tm_scale_bar(position = c("right", "bottom"), text.size = 0.5) +
  tm_layout(title = "Distance from Jesuit Missions, Negative Exponential", 
            title.bg.color = "white", 
            legend.position = c("left", "bottom"),
            legend.bg.color = "white", 
            frame = FALSE,
            fontfamily = "Lato")
```

By adjusting $\lambda$, we can make the decay more or less “aggressive.” With our present specification, it is “between” the linear and logarithmic cases.

Lastly, we could consider using a **Gaussian** decay function, like this:

$$
d: \mathbb{R} \to \mathbb{R}, \quad x \mapsto \mathrm{exp}\left(-\frac{x^2}{2\sigma^2}\right)
$$
Letting $\sigma=100$ gives the following:

```{r, echo = FALSE, out.width = "50%", fig.align = "center"}
ggplot() +
  geom_function(col = "#995082", size = 1.5, fun = function(x) exp(-(x^2)/(2*100^2))) +
  xlim(0, 1000) +
  labs(x = "x", y = "d(x)") +
  theme_minimal() +
  theme(text = element_text(family = "Lato"))
```

We can see that by using the Gaussian decay function, we can model something that is closer to a “cutoff.” Applying this function to the data gives the following map:

```{r, echo = FALSE, out.width = "70%", fig.align = "center", dev = "png", dpi = 300}
tm_shape(mutate(spli, distmissx = exp(-(distmiss^2)/(2*100^2)))) + 
  tm_fill("distmissx", palette = "Purples", style = "equal", n = 10) + 
  tm_borders(col = "#BBBBBB") + 
  tm_shape(shp1) + 
  tm_borders(col = "#444444") + 
  tm_text("NAME_1", size = 0.75, col = "black", bg.color = "#FFFFFF") +
  tm_shape(shp0) + 
  tm_borders(col = "#000000") +
  tm_compass(position = c("right", "top"), size = 2) +
  tm_scale_bar(position = c("right", "bottom"), text.size = 0.5) +
  tm_layout(title = "Distance from Jesuit Missions, Gaussian", 
            title.bg.color = "white", 
            legend.position = c("left", "bottom"),
            legend.bg.color = "white", 
            frame = FALSE,
            fontfamily = "Lato")
```

We can clearly see that different functions yield vastly different distance measures across municipalities. So it is quite sensible to suspect that estimation results are subject to the choice of distance measure. To check this, we run the regression from column (2) of Table 2 again -- but each time with a different distance measure.

```{r}
litrd <- litr %>%
  mutate(distmiss_log = log(distmiss),
         distmiss_exp = exp(-distmiss/200),
         distmiss_gau = exp(-(distmiss^2)/(2*100^2)))

col2_reg <- lm(illiteracy ~ distmiss + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr + ita + mis + mis1, data = litrd)
col2_log <- lm(illiteracy ~ distmiss_log + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr + ita + mis + mis1, data = litrd)
col2_exp <- lm(illiteracy ~ distmiss_exp + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr + ita + mis + mis1, data = litrd)
col2_gau <- lm(illiteracy ~ distmiss_gau + lati + longi + area + tempe + alti + preci + rugg + river + coast + corr + ita + mis + mis1, data = litrd)
```

```{r, echo = FALSE, results='asis'}
cat("\\begin{center}")
cat("\\scalebox{.8}{")
stargazer(col2_reg, col2_log, col2_exp, col2_gau,
          type = "latex",
          se = starprep(col2_reg, col2_log, col2_exp, col2_gau, se_type = "stata"),
          omit.stat = "f",
          float = FALSE,
          header=FALSE)
cat("}")
cat("\\end{center}")
```

Of course, coefficient magnitudes cannot be directly compared, since we wildly scaled and transformed the distance measure. Even the sign changes do not necessarily need to worry us, since we transformed distance measures in a way that they sometimes shrank and sometimes increased with increasing distance. However, we see that using the different distance measures, effect sizes are no longer statistically different from zero.

## Persistence and Space, or: Where's Waldo?

Because any text about spatial autocorrelation would be incomplete without reciting Waldo\footnote{Here he is.} Tobler's First Law of Geography, here it is in all its beauty: “Everything is related to everything else, but near things are more related than distant things.” Applied to the present context, this has important implications.

First, the presence of a mission in some given municipality is likely autocorrelated with whether adjacent municipalities were subject to missionary activity too. If the first missionaries went to some place in what we today call Misiones, it is likely that subsequent missionaries built on locally specific knowledge of earlier missions and settled near them. Our distance plots, especially the logarithmic one, are a good indication of that autocorrelation, since we can see that all missions were more or less in the same region. 

Second, we can assume literacy rates to be spatially autocorrelated as well. Again, we can see this from the map we originally produced where we plotted literacy rates across the regions analyzed by \textcite{valencia2019}. We can see that high literacy rates concentrate in one region around the Argentinian state of Misiones and the northwestern part of Rio Grande do Sul, and in one region in the eastern part of Rio Grande do Sul. And if we take a glance at the map below, we can also see that these regions are clusters of higher population density.

\textcite{kelly2021} notes that this covert relation leads to standard errors usually being too low since the presence of spatial clusters in two variables means that whatever correlation exists between the variables in one locality is likely to also be present in adjacent localities. He also notes that different types of available standard error corrections lead to widely different results. Conley standard errors, which \textcite{valencia2019} uses as a robustness check, tend to be the most conservative of the corrections. Having standard errors that are too small, of course, does not bias the estimate, but will cause false positive results. 

The same can be thought of if there are potential confounders. If there is a unobserved spatial variable that influences the outcome and that is correlated with the covariates that are included in the model, the coefficient estimates will be biased \parencite{dupont2023}. Assuming the variables considered in the model in \textcite{valencia2019} behave like most other socioeconomic variables in space, we can assume that any spatial effect will be correlated with the model covariates, leading to the possibility of biased coefficients. 

\newpage

# Task C

## The Perils of Ignoring Peer Effects

In “The perils of peer effects”, \textcite{angrist2014} claims that without covariates, peer effects are vacuous and designs that “manipulate peer characteristics […] have mostly uncovered little in the way of socially significant causal effects” \parencite{angrist2014} While these assumptions may hold in theory and under the assumptions of IV, it is important to consider the potential impact of personal effects. Neglecting to do so could result in implications that may compromise the validity of a method or overlook important social dynamics. Peer groups hold significant power within society, from shaping individual behaviors to influencing broader social outcomes. It is important to acknowledge the complexity of social phenomena and avoid reducing them to mere statistical forces. Oversimplifying social dynamics and human behavior can be a risk when doing so.

In educational research, peer effects are an important consideration, particularly within experimental contexts. Several studies confirm that peers can have a significant impact on an individual's academic performance. This is especially evident in detailed analyses that account for variations in individual achievement and the nature of peer group modifications under examination. Moreover, it is often noted in academic literature on higher education that there are significant peer effects, particularly in outcomes that have a more “social” dimension. In \parencite{sacerdote2011}, there is various literature listed that found profound outcomes on social outcomes such as effects in drinking behavior whilst the peer effects on test scores \parencite[see][]{lyle2007, carrell2009, sacerdote2011}.

Moreover, it is important to consider peer effects in research designs to maintain a balance between internal and external validity. While researchers prioritize internal validity in experimental settings to establish causality, neglecting peer effects can compromise external validity and limit the generalizability of findings to real-world contexts where peer influence is prevalent. For instance, it is possible that the outcomes of an experimental intervention conducted in a controlled laboratory environment may differ from those obtained when implementing the same intervention in an actual classroom setting where peer interactions play a significant role. Research in social psychology and sociology consistently demonstrates the profound impact of social norms, peer pressure, and group dynamics on individual behavior. It is important to acknowledge that statistical forces have behavioral implications, despite some potential arguments to the contrary.

Network dependence, which includes both spatial and social dependencies, is a crucial factor that can significantly impact the validity and relevance of research or analysis. It is important to consider spatial dependence to avoid biased estimation. For instance, in the education example mentioned earlier, neglecting social dependencies can result in underestimating the impact of peer influence on behaviors of individuals. This may lead to ineffective or inefficient policy interventions. In the educational context, ignoring these “social peer effects” may lead to misguided strategies that cannot manage to address the underlying social influences on student outcomes.

In conclusion, whilst Angrist raises valid points in his work, one must take into account the complexity of social structures and network dynamics. One also must acknowledge that social dynamics impact the behavior of individuals. In education for example, we showed that there more or less agreement in the existing literature that peer effects influence test scores moderately whilst peer effects in determining social outcomes are far more widespread. While Angrist's assertions regarding the necessity of covariates and the limited findings of designs manipulating peer characteristics provide valuable insights, it's essential to recognize the potential influence of personal effects and the broader context of social phenomena. Peer groups wield substantial power within society, shaping behaviors and influencing outcomes, underscoring the importance of considering peer effects in research designs.


\newpage

# Task D

The image is a screenshot, and those are conventionally stored in PNG format. The photo _contained_ in the screenshot is a photograph, and it is difficult to guess which format it was originally saved in. Let's say it's JPEG. Then, someone inserted the image into the assignment PDF, meaning it is technically not stored as a PNG anymore. What all those ways of storing the image have in common is that they are **raster formats**, as they consist of individual pixels. And even if we print the document containing the image, it gets printed as dots, which are not exactly pixels, but certainly form a raster rather than a vector.

\newpage
